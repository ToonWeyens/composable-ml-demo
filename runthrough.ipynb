{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting info from blueprints through custom tasks and composable ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dr.Client(config_path = \"/Volumes/GoogleDrive/My Drive/Configurations/drconfig_staging.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or get model to work from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALERT</th>\n",
       "      <th>SAR</th>\n",
       "      <th>kycRiskScore</th>\n",
       "      <th>income</th>\n",
       "      <th>tenureMonths</th>\n",
       "      <th>creditScore</th>\n",
       "      <th>state</th>\n",
       "      <th>nbrPurchases90d</th>\n",
       "      <th>avgTxnSize90d</th>\n",
       "      <th>totalSpend90d</th>\n",
       "      <th>...</th>\n",
       "      <th>indCustReqRefund90d</th>\n",
       "      <th>totalRefundsToCust90d</th>\n",
       "      <th>nbrPaymentsCashLike90d</th>\n",
       "      <th>maxRevolveLine</th>\n",
       "      <th>indOwnsHome</th>\n",
       "      <th>nbrInquiries1y</th>\n",
       "      <th>nbrCollections3y</th>\n",
       "      <th>nbrWebLogins90d</th>\n",
       "      <th>nbrPointRed90d</th>\n",
       "      <th>PEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>110300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>757</td>\n",
       "      <td>PA</td>\n",
       "      <td>10</td>\n",
       "      <td>153.80</td>\n",
       "      <td>1538.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>45.82</td>\n",
       "      <td>5</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>107800.0</td>\n",
       "      <td>6</td>\n",
       "      <td>715</td>\n",
       "      <td>NY</td>\n",
       "      <td>22</td>\n",
       "      <td>1.59</td>\n",
       "      <td>34.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>67.40</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>751</td>\n",
       "      <td>MA</td>\n",
       "      <td>7</td>\n",
       "      <td>57.64</td>\n",
       "      <td>403.48</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>450.69</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "      <td>NJ</td>\n",
       "      <td>14</td>\n",
       "      <td>29.52</td>\n",
       "      <td>413.28</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>71.43</td>\n",
       "      <td>0</td>\n",
       "      <td>8000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>709</td>\n",
       "      <td>PA</td>\n",
       "      <td>54</td>\n",
       "      <td>115.77</td>\n",
       "      <td>6251.58</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2731.39</td>\n",
       "      <td>3</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ALERT  SAR  kycRiskScore    income  tenureMonths  creditScore state  \\\n",
       "0      1    0             3  110300.0             5          757    PA   \n",
       "1      1    0             2  107800.0             6          715    NY   \n",
       "2      1    0             1   74000.0            13          751    MA   \n",
       "3      1    0             0   57700.0             1          659    NJ   \n",
       "4      1    0             1   59800.0             3          709    PA   \n",
       "\n",
       "   nbrPurchases90d  avgTxnSize90d  totalSpend90d  ... indCustReqRefund90d  \\\n",
       "0               10         153.80        1538.00  ...                   1   \n",
       "1               22           1.59          34.98  ...                   1   \n",
       "2                7          57.64         403.48  ...                   1   \n",
       "3               14          29.52         413.28  ...                   1   \n",
       "4               54         115.77        6251.58  ...                   1   \n",
       "\n",
       "   totalRefundsToCust90d  nbrPaymentsCashLike90d  maxRevolveLine  indOwnsHome  \\\n",
       "0                  45.82                       5            6000            0   \n",
       "1                  67.40                       0           10000            1   \n",
       "2                 450.69                       0           10000            0   \n",
       "3                  71.43                       0            8000            1   \n",
       "4                2731.39                       3            7000            1   \n",
       "\n",
       "   nbrInquiries1y  nbrCollections3y  nbrWebLogins90d  nbrPointRed90d  PEP  \n",
       "0               3                 0                6               1    0  \n",
       "1               3                 0               87               0    0  \n",
       "2               3                 0                6               0    0  \n",
       "3               5                 0                7               2    0  \n",
       "4               1                 0                8               1    0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read from a local file, uncomment and use:\n",
    "# df = pd.read_csv('./data/DR_Demo_AML_Alert.csv')\n",
    "\n",
    "# To read from an s3 bucket:\n",
    "df = pd.read_csv(\n",
    "    \"https://s3.amazonaws.com/datarobot_public_datasets/DR_Demo_AML_Alert.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "project_id = \"62bb09db63886fb9c1668bff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Project URL: https://staging.datarobot.com/projects/62bb09db63886fb9c1668bff/eda\n",
      "Custom Project ID: 62bb09db63886fb9c1668bff\n"
     ]
    }
   ],
   "source": [
    "if do_training:\n",
    "    # Create a project by uploading data. It will take a few moments.\n",
    "    project = dr.Project.create(\n",
    "        sourcedata=df,\n",
    "        project_name=\"DR_Demo_API_alert_AML_simple_{}\".format(\n",
    "            pd.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        ),\n",
    "    )\n",
    "    # Set the project's target and initiate AutoPilot in Quick mode\n",
    "    project.set_target(\n",
    "        target=\"SAR\",\n",
    "        mode=\"quick\",\n",
    "        worker_count=4,\n",
    "        advanced_options=dr.AdvancedOptions(seed=RANDOM_SEED),\n",
    "    )\n",
    "\n",
    "    # Open project's Leaderboard to monitor the progress in UI\n",
    "    #project.open_leaderboard_browser()\n",
    "\n",
    "    # Wait for AutoPilot to finish. You can set verbosity to 0 if you do not wish to see progress updates\n",
    "    #project.wait_for_autopilot(verbosity=1)\n",
    "else:\n",
    "    # To access an existing project set your project ID below\n",
    "    project = dr.Project.get(project_id)\n",
    "\n",
    "print(\n",
    "    \"Custom Project URL: \" + \"https://staging.datarobot.com/projects/\" + project.id + \"/eda\"\n",
    ")\n",
    "print(\"Custom Project ID: \" + project.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract blueprint from existing model in code through GUI\n",
    "https://staging.datarobot.com/projects/62bb09db63886fb9c1668bff/models/62bb0b5313653002cba74a56/blueprint\n",
    "![view code in GUI](images/view_code_in_gui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = Workshop(user_blueprint_id='62bc05dee13965db36397b1d', project_id='62bb09db63886fb9c1668bff')\n",
      "\n",
      "csrnotes = w.Features.csrNotes\n",
      "\n",
      "pnia4 = w.Tasks.PNIA4(w.TaskInputs.NUM)\n",
      "\n",
      "ordcat2 = w.Tasks.ORDCAT2(w.TaskInputs.CAT)\n",
      "ordcat2.set_task_parameters(m='random')\n",
      "\n",
      "wngec2 = w.Tasks.WNGEC2(csrnotes, output_method=w.TaskOutputMethod.STACK)\n",
      "wngec2.set_task_parameters(bi=True, a=0, lc=True, madf=0.8, midf=2, nrm='l2', num=[1, 2], tol=0, uidf=False)\n",
      "\n",
      "rfc = w.Tasks.RFC(wngec2, pnia4, ordcat2)\n",
      "rfc.set_task_parameters(e='RandomForestClassifier', mf=[0.2, 0.3, 0.4], ml=2000, ls=[5, 10, 20])\n",
      "\n",
      "rfc_blueprint = w.BlueprintGraph(rfc, name='RandomForest Classifier (Gini)')\n"
     ]
    }
   ],
   "source": [
    "from datarobot_bp_workshop import Workshop\n",
    "blueprint_id = '2130d16744eed608647655972974b6a6'\n",
    "\n",
    "w = Workshop(project_id = project_id)\n",
    "blueprint_graph = w.clone(blueprint_id=blueprint_id, project_id=project_id)\n",
    "source_code = blueprint_graph.to_source_code(to_stdout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom task to dump output if it does not yet exist\n",
    "\n",
    "See notebook `create_new_custom_task.ipynb`\n",
    "\n",
    "Come back when you're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dump_output: [CUSTOMT_62bb237ddfb81eb3d30e8ba7] \n",
       "  - (No description)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.search_tasks('dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_ct = w.CustomTasks.CUSTOMT_62bb237ddfb81eb3d30e8ba7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Workshop(project_id=project_id)\n",
    "\n",
    "csrnotes = w.Features.csrNotes\n",
    "\n",
    "pnia4 = w.Tasks.PNIA4(w.TaskInputs.NUM)\n",
    "pnia4 = dump_ct(pnia4)\n",
    "\n",
    "ordcat2 = w.Tasks.ORDCAT2(w.TaskInputs.CAT)\n",
    "ordcat2.set_task_parameters(m='random')\n",
    "\n",
    "wngec2 = w.Tasks.WNGEC2(csrnotes, output_method=w.TaskOutputMethod.STACK)\n",
    "wngec2.set_task_parameters(bi=True, a=0, lc=True, madf=0.8, midf=2, nrm='l2', num=[1, 2], tol=0, uidf=False)\n",
    "\n",
    "rfc = w.Tasks.RFC(wngec2, pnia4, ordcat2)\n",
    "rfc.set_task_parameters(e='RandomForestClassifier', mf=[0.2, 0.3, 0.4], ml=2000, ls=[5, 10, 20])\n",
    "\n",
    "rfc_blueprint = w.BlueprintGraph(rfc, name='RandomForest Classifier (Gini) with output dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some information about tasks if you're interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCustom\u001b[0m\n",
      "\n",
      "  - dump_output (CUSTOMT_62bb237ddfb81eb3d30e8ba7)\n",
      "\u001b[34mPreprocessing\u001b[0m\n",
      "\n",
      "  \u001b[34mNumeric Preprocessing\u001b[0m\n",
      "\n",
      "    \u001b[34mData Quality\u001b[0m\n",
      "\n",
      "      - Missing Values Imputed (arbitrary or quick median) (PNIA4)\n",
      "      - Numeric Data Cleansing (NDC)\n",
      "    \u001b[34mDimensionality Reducer\u001b[0m\n",
      "\n",
      "      - Truncated Singular Value Decomposition (SVD2)\n",
      "      - Partial Principal Components Analysis (PPCA)\n",
      "      - Truncated Singular Value Decomposition (SVD)\n",
      "    \u001b[34mScaling\u001b[0m\n",
      "\n",
      "      - Log Transformer (LOGT)\n",
      "      - Transparent Search for best transformation (BTRANSF6T)\n",
      "      - Search for best transformation including Smooth Ridit (BTRANSF6)\n",
      "      - Transform on the link function scale (LINK)\n",
      "      - Impose Uniform Transform (UNIF3)\n",
      "      - Ridit Transform (SRDT3)\n",
      "      - Smooth Ridit Transform (RDT5)\n",
      "      - Standardize (ST)\n",
      "      - Standardize (RST)\n",
      "    - Normalizer (NORM)\n",
      "    - One-Hot Encoding (PDM3)\n",
      "    - Constant Splines (GS)\n",
      "    - Search for ratios (RATIO3)\n",
      "    - Binning of numerical variables (BINNING)\n",
      "    - Search for differences (DIFF3)\n",
      "    - Missing Values Imputed (quick median) (PNI2)\n",
      "    - Sparse Interaction Machine (SPOLY)\n",
      "  \u001b[34mCategorical Preprocessing\u001b[0m\n",
      "\n",
      "    - Ordinal encoding of categorical variables (ORDCAT2)\n",
      "    - Category Count (PCCAT)\n",
      "    - Buhlmann credibility estimates for high cardinality features (CRED1)\n",
      "    - One-Hot Encoding (PDM3)\n",
      "    - Categorical Embedding (CATEMB)\n",
      "    - Univariate credibility estimates with L2 (CRED1b1)\n",
      "  \u001b[34mText Preprocessing\u001b[0m\n",
      "\n",
      "    - SpaCy Named Entity Recognition Detector (SPACY_NAMED_ENTITY_RECOGNITION)\n",
      "    - NLTK Sentiment Featurizer (NLTK_SENTIMENT)\n",
      "    - Keras Byte-Pair Encoding (BPE) of text variables (KERAS_BPE_TOKENIZER_V2)\n",
      "    - Pretrained TinyBERT Featurizer (TINYBERTFEA)\n",
      "    - Keras Byte-Pair Encoding (BPE) of text variables (KERAS_BPE_TOKENIZER)\n",
      "    - Fasttext Word Vectorization and Mean text embedding (TXTEM1)\n",
      "    - Keras encoding of text variables (KERAS_TOKENIZER)\n",
      "    - TextBlob Sentiment Featurizer (TEXTBLOB_SENTIMENT)\n",
      "    - Matrix of word-grams occurrences (PTM3)\n",
      "  \u001b[34mImage Preprocessing\u001b[0m\n",
      "\n",
      "    - Pretrained Multi-Level Global Average Pooling Image Featurizer (IMGFEA)\n",
      "    - OpenCV Image Featurizer (OPENCV_FEATURIZER)\n",
      "    - No Post Processing (IMAGE_POST_PROCESSOR)\n",
      "    - OpenCV Detect Largest Rectangle (OPENCV_DETECT_LARGEST_RECTANGLE)\n",
      "    - Grayscale Downscaled Image Featurizer (IMG_GRAYSCALE_DOWNSCALED_IMAGE_FEATURIZER)\n",
      "  \u001b[34mSummarized Categorical Preprocessing\u001b[0m\n",
      "\n",
      "    - Single Column Converter for Summarized Categorical (SCBAGOFCAT2)\n",
      "    - Summarized Categorical to Sparse Matrix (CDICT2SP)\n",
      "  \u001b[34mGeospatial Preprocessing\u001b[0m\n",
      "\n",
      "    - Geospatial Location Converter (GEO_IN)\n",
      "    - Spatial Neighborhood Featurizer (GEO_NEIGHBOR_V1)\n",
      "\n",
      "\u001b[34mModels\u001b[0m\n",
      "\n",
      "  \u001b[34mRegression\u001b[0m\n",
      "\n",
      "    \u001b[34mBoosted Regression\u001b[0m\n",
      "\n",
      "      - eXtreme Gradient Boosted Trees Regressor (Boosted Predictions) (XL_XGBR2)\n",
      "      - Light Gradient Boosting on ElasticNet Predictions (RES_PLGBMTR)\n",
      "      - Text fit on Residuals (L1 /  Least-Squares Loss) (XL_ENETCDWC)\n",
      "      - Text fit on Residuals (L1 /  Least-Squares Loss) (RES_FDENETCD)\n",
      "      - eXtreme Gradient Boosting Regression on ElasticNet Predictions (RES_ESXGBR2)\n",
      "      - Light Gradient Boosting Regression on ElasticNet Predictions (RES_ESLGBMTR)\n",
      "      - Text fit on Residuals (L1 /  Least-Squares Loss) (XL_ENETCD)\n",
      "      - eXtreme Gradient Boosted Trees Regressor (XL_PXGBR2)\n",
      "      - eXtreme Gradient Boosting on ElasticNet Predictions (RES_XGBR2)\n",
      "      - eXtreme Gradient Boosted Trees Regressor with Early Stopping (XL_ESXGBR2)\n",
      "    - Elastic-Net Regressor (L1 / Least-Squares Loss) with Binned numeric features (BENETCD2)\n",
      "    - Gaussian Process Regressor with Radial Basis Function Kernel (GPRRBF)\n",
      "    - Auto-tuned K-Nearest Neighbors Regressor (Euclidean Distance) (KNNR)\n",
      "    - Gradient Boosted Trees Quantile Regressor (QGBR2)\n",
      "    - Gaussian Process Regressor with Rational Quadratic Kernel (GPRRQ)\n",
      "    - Adaboost Regressor (ABR)\n",
      "    - Regularized Quantile Regressor with Keras (KERAS_REGULARIZED_QUANTILE_REG)\n",
      "    - Light Gradient Boosted Trees Regressor with Early Stopping (ESLGBMTR)\n",
      "    - Frequency-Severity ElasticNet (FSEE)\n",
      "    - Nystroem Kernel SVM Regressor using Elastic-Net (ASVMER)\n",
      "    - Gaussian Process Regressor with Exponential Sine Squared Kernel (GPRESS)\n",
      "    - Gradient Boosted Trees Regressor with Early Stopping (Least-Squares Loss) (ESGBR2)\n",
      "    - ExtraTrees Regressor (Shallow) (SHAPRFR)\n",
      "    - Keras Text Convolutional Neural Network Regressor (KERAS_CNN_TEXT_R)\n",
      "    - Elastic-Net Regressor (L1 / Least-Squares Loss) with K-Means Distance Features (KMDENETCD)\n",
      "    - Eureqa Generalized Additive Model (EQ_ESXGBR)\n",
      "    - eXtreme Gradient Boosted Trees Regressor (PXGBR2)\n",
      "    - Frequency-Severity Generalized Additive Model (FSGG2)\n",
      "    - XRuleFit Regressor (XRULEFITR)\n",
      "    - Elastic-Net Regressor (L1 / Least-Squares Loss) (ENETCDWC)\n",
      "    - eXtreme Gradient Boosted Trees Quantile Regressor with Early Stopping (ESQUANTXGBR)\n",
      "    - eXtreme Gradient Boosted Trees Regressor with Early Stopping and Unsupervised Learning Features (UESXGBR2)\n",
      "    - Auto-tuned Stochastic Gradient Descent Regression (SGDRA)\n",
      "    - eXtreme Gradient Boosted Trees Regressor with Early Stopping (ESXGBR2)\n",
      "    - Support Vector Regressor (Radial Kernel) (SVMR2)\n",
      "    - Eureqa Regressor (EQR)\n",
      "    - Gradient Boosted Trees Quantile Regressor with Early Stopping (QESGBR2)\n",
      "    - Auto-Tuned N-Gram Text Regressor using token counts (WNGER2)\n",
      "    - Lasso Regression (LASSO2)\n",
      "    - Dropout Additive Regression Trees Regressor (PLGBMDR)\n",
      "    - Auto-Tuned Char N-Gram Text Modeler using token counts (CNGER2)\n",
      "    - Keras Neural Network Regressor (KERASR)\n",
      "    - Gaussian Process Regressor with Dot Product Kernel (GPRDP)\n",
      "    - Partial Least-Squares Regression (PLS)\n",
      "    - Linear Regression (GLMCD)\n",
      "    - Auto-Tuned Summarized Categorical Regressor (SCENETR)\n",
      "    - Stochastic Gradient Descent Regression (SGDR)\n",
      "    - Ridge Regression (RIDGEWC)\n",
      "    - Nystroem Kernel SVM Regressor using Logistic Regression (ASVMSKR)\n",
      "    - Frequency-Cost ElasticNet (FCEE)\n",
      "    - Gaussian Process Regressor with Matern Kernel (GPRM)\n",
      "    - Statsmodels Quantile Regressor (QUANTILER)\n",
      "    - Hot Spots (XPRIMR)\n",
      "    - eXtreme Gradient Boosted Trees Regressor (XGBR2)\n",
      "    - Ridge Regression (RIDGE)\n",
      "    - Elastic-Net Regressor (L1 / Least-Squares Loss) with Unsupervised Learning Features (UENETCD)\n",
      "    - Frequency-Severity Light Gradient Boosted Trees (FSLL)\n",
      "    - LightGBM Random Forest Regressor (PLGBMRFR)\n",
      "    - Frequency-Severity eXtreme Gradient Boosted Trees (FSXX2)\n",
      "    - Elastic-Net Regressor (L1 / Least-Squares Loss) (ENETCD)\n",
      "    - RuleFit Regressor (RULEFITR)\n",
      "    - ExtraTrees Regressor (RFR)\n",
      "  \u001b[34mBinary Classification\u001b[0m\n",
      "\n",
      "    \u001b[34mBoosted Classification\u001b[0m\n",
      "\n",
      "      - Light Gradient Boosting on ElasticNet Predictions (RES_PLGBMTC)\n",
      "      - eXtreme Gradient Boosted Trees Classifier (Boosted Predictions) (XL_XGBC2)\n",
      "      - eXtreme Gradient Boosting on ElasticNet Predictions (RES_XGBC2)\n",
      "      - Light Gradient Boosting Classification on ElasticNet Predictions (RES_ESLGBMTC)\n",
      "      - Text fit on Residuals (L1 / Binomial Deviance) (XL_LENETCDWC)\n",
      "      - Text fit on Residuals (L1 / Binomial Deviance) (XL_LENETCD)\n",
      "    - Elastic-Net Classifier (L1 / Binomial Deviance) (LENETCDWC)\n",
      "    - Partial Least-Squares Classification (PLSC)\n",
      "    - XRuleFit Classifier (XRULEFITC)\n",
      "    - eXtreme Gradient Boosted Trees Classifier (XGBC2)\n",
      "    - Elastic-Net Classifier (L1 / Binomial Deviance) (LENETCD)\n",
      "    - eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features (UESXGBC2)\n",
      "    - Multinomial Naive Bayes classifier (scikit-learn) (MNBC)\n",
      "    - RuleFit Classifier (RULEFITC)\n",
      "    - Hot Spots (XPRIMC)\n",
      "    - LightGBM Random Forest Classifier (PLGBMRFC)\n",
      "    - Gaussian Process Classifier with Matern Kernel (GPCM)\n",
      "    - Logistic Regression (LR)\n",
      "    - Eureqa Classifier (EQC)\n",
      "    - eXtreme Gradient Boosted Trees Classifier (PXGBC2)\n",
      "    - Fine-Tuned Image Classifier (All Layers) (IMGFTC)\n",
      "    - Gaussian Naive Bayes classifier (scikit-learn) (GNBC)\n",
      "    - Support Vector Classifier (Radial Kernel) (SVMC2)\n",
      "    - Light Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features (UESLGBMTC)\n",
      "    - Keras Text Convolutional Neural Network Binary Classifier (KERAS_CNN_TEXT_C)\n",
      "    - Dropout Additive Regression Trees Classifier (PLGBMDC)\n",
      "    - Gradient Boosted Trees Classifier with Early Stopping (ESGBC)\n",
      "    - Fine-Tuned Multi-Image Regressor (All Layers) (MULTIIMGFTR)\n",
      "    - Logistic Regression (LRCD)\n",
      "    - Auto-Tuned Summarized Categorical Classifier (SCLENETC)\n",
      "    - Elastic-Net Classifier (L1 / Binomial Deviance) with K-Means Distance Features (KMDLENETCD)\n",
      "    - Elastic-Net Classifier with Naive Bayes Feature Weighting (NB_LENETCD)\n",
      "    - Eureqa Generalized Additive Model Classifier (EQ_ESXGBC)\n",
      "    - Gaussian Process Classifier with Radial Basis Function Kernel (GPCRBF)\n",
      "    - Fine-Tuned Multi-Image Classifier (All Layers) (MULTIIMGFTC)\n",
      "    - Keras Neural Network Classifier (KERASC)\n",
      "    - Nystroem Kernel SVM Classifier using Elastic-Net (ASVMEC)\n",
      "    - eXtreme Gradient Boosted Trees Classifier with Early Stopping (ESXGBC2)\n",
      "    - ExtraTrees Classifier (Gini) (RFC)\n",
      "    - Nystroem Kernel SVM Classifier using Logistic Regression (ASVMSKC)\n",
      "    - Auto-Tuned N-Gram Text Classifier using token counts (WNGEC2)\n",
      "    - ExtraTrees Classifier (Gini) (SHAPRFC)\n",
      "    - Adaboost Classifier (ABC)\n",
      "    - Fine-Tuned Image Regressor (All Layers) (IMGFTR)\n",
      "    - Bernoulli Naive Bayes classifier (scikit-learn) (BNBC)\n",
      "    - Auto-Tuned Char N-Gram Text Modeler using token counts (CNGEC2)\n",
      "    - Regularized Logistic Regression (L2) (LR1)\n",
      "    - Auto-tuned K-Nearest Neighbors Classifier (Euclidean Distance) (KNNC)\n",
      "    - Light Gradient Boosted Trees Classifier with Early Stopping (ESLGBMTC)\n",
      "    - Stochastic Gradient Descent Classifier (SGDC)\n",
      "  \u001b[34mMulti-class Classification\u001b[0m\n",
      "\n",
      "    \u001b[34mBoosted Multi-class\u001b[0m\n",
      "\n",
      "      - Light Gradient Boosting on ElasticNet Predictions (RES_PLGBMTC)\n",
      "      - Light Gradient Boosting Classification on ElasticNet Predictions (RES_ESLGBMTC)\n",
      "    - eXtreme Gradient Boosted Trees Classifier (XGBC2)\n",
      "    - Elastic-Net Classifier (L1 / Binomial Deviance) (LENETCD)\n",
      "    - LightGBM Random Forest Classifier (PLGBMRFC)\n",
      "    - Logistic Regression (LR)\n",
      "    - eXtreme Gradient Boosted Trees Classifier (PXGBC2)\n",
      "    - Fine-Tuned Image Classifier (All Layers) (IMGFTC)\n",
      "    - Light Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features (UESLGBMTC)\n",
      "    - Keras Text Convolutional Neural Network Multi-class Classifier (KERAS_CNN_TEXT_MULTIC)\n",
      "    - Dropout Additive Regression Trees Classifier (PLGBMDC)\n",
      "    - Gradient Boosted Trees Classifier with Early Stopping (ESGBC)\n",
      "    - Fine-Tuned Multi-Image Regressor (All Layers) (MULTIIMGFTR)\n",
      "    - Logistic Regression (LRCD)\n",
      "    - Fine-Tuned Multi-Image Classifier (All Layers) (MULTIIMGFTC)\n",
      "    - eXtreme Gradient Boosted Trees Classifier with Early Stopping (ESXGBC2)\n",
      "    - ExtraTrees Classifier (Gini) (RFC)\n",
      "    - ExtraTrees Classifier (Gini) (SHAPRFC)\n",
      "    - Fine-Tuned Image Regressor (All Layers) (IMGFTR)\n",
      "    - Regularized Logistic Regression (L2) (LR1)\n",
      "    - Keras Neural Network Classifier (KERASMULTIC)\n",
      "    - Light Gradient Boosted Trees Classifier with Early Stopping (ESLGBMTC)\n",
      "    - Stochastic Gradient Descent Classifier (SGDC)\n",
      "  \u001b[34mMulti-label Classification\u001b[0m\n",
      "\n",
      "    - Keras Text Convolutional Neural Network Multi-label Classifier (KERAS_CNN_TEXT_MULTILABELC)\n",
      "  \u001b[34mUnsupervised\u001b[0m\n",
      "\n",
      "    \u001b[34mAnomaly Detection\u001b[0m\n",
      "\n",
      "      - One-Class SVM Anomaly Detection with Calibration (ADOSVM_CAL)\n",
      "      - Keras Autoencoder with Calibration (KERAS_AUTOENCODER_CAL)\n",
      "      - Keras Autoencoder (KERAS_AUTOENCODER)\n",
      "      - Isolation Forest Anomaly Detection (ADISOFOR)\n",
      "      - Mahalanobis Distance Ranked Anomaly Detection with PCA and Calibration (ADMAHAL_PCA_CAL)\n",
      "      - Mahalanobis Distance Ranked Anomaly Detection with PCA (ADMahalPCA)\n",
      "      - Double Median Absolute Deviation Anomaly Detection with Calibration (ADDMAD_CAL)\n",
      "      - One-Class SVM Anomaly Detection (ADOSVM)\n",
      "      - Keras Variational Autoencoder (KERAS_VARIATIONAL_AUTOENCODER)\n",
      "      - Anomaly Detection with Supervised Learning (XGB) and Calibration (ADXGB2_CAL)\n",
      "      - Local Outlier Factor Anomaly Detection (ADLOF)\n",
      "      - Local Outlier Factor Anomaly Detection with Calibration (ADLOF_CAL)\n",
      "      - Anomaly Detection with Supervised Learning (XGB) (ADXGB)\n",
      "      - Double Median Absolute Deviation Anomaly Detection (ADDMAD)\n",
      "      - Keras Variational Autoencoder with Calibration (KERAS_VARIATIONAL_AUTOENCODER_CAL)\n",
      "      - Isolation Forest Anomaly Detection with Calibration (ADISOFOR_CAL)\n",
      "    \u001b[34mClustering\u001b[0m\n",
      "\n",
      "      - K-Means Clustering (KMEANS)\n",
      "  \n",
      "\n",
      "\u001b[34mCalibration\u001b[0m\n",
      "\n",
      "  - Calibrate predictions (CALIB2)\n",
      "  - Calibrate predictions: Platt (PLACAL2)\n",
      "  - Calibrate predictions (CALIB)\n",
      "  - Fit High Cardinality and Text Classifier (XLF_LENETCD)\n",
      "  - Fit High Cardinality Categorical and Text Regressor (XLF_ENETCD)\n",
      "  - Calibrate predictions with RF (CALIB_V2_RFC)\n",
      "  - Calibrate predictions: Weighted Calibration (SWCAL)\n",
      "\u001b[34mColumn Selection\u001b[0m\n",
      "\n",
      "  - Converter for Text Mining (SCTXT4)\n",
      "  - Converter for Text Mining (SCTXT2)\n",
      "  - Single Column Converter (SCPICK)\n",
      "  - Single Column Converter (SCPICK2)\n",
      "  - Single Column Converter for Summarized Categorical (SCBAGOFCAT)\n",
      "  - Multiple Column Selector (MCPICK)\n",
      "\u001b[34mOther\u001b[0m\n",
      "\n",
      "  \u001b[34mAutomatic Feature Selection\u001b[0m\n",
      "\n",
      "    - Feature Selection for Ratios/Differences (FS_RFR2)\n",
      "    - Feature Selection with Lasso Pre-selection (classification) (FS_RFCDR_LASSO)\n",
      "    - Feature Selection for dimensionality reduction (FS_RFCDR2)\n",
      "    - Feature Selection using L1 Regularization and Predictions as Offset (FS_XL_LASSO2)\n",
      "    - Feature Selection for Ratios/Differences (FS_RFC2)\n",
      "    - Feature Selection for dimensionality reduction (FS_RFRDR2)\n",
      "    - Rare Feature Masking (RFMASK)\n",
      "    - Feature Selection with Lasso Pre-selection (regression) (FS_RFRDR_LASSO)\n",
      "  - Extract Trend Features (TRENDFEATS)\n",
      "  - Bind branches (BIND)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tasks registered in DataRobot\n",
    "w.list_categories(show_tasks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://staging.datarobot.com/model-docs/tasks/WNGEC2-Auto-Tuned-N-Gram-Text-Modeler-using-token-counts.html\n",
      "Help on WNGEC2 in module datarobot_bp_workshop.factories object:\n",
      "\n",
      "class WNGEC2(datarobot_bp_workshop.friendly_repr.FriendlyRepr)\n",
      " |  Auto-Tuned N-Gram Text Classifier using token counts\n",
      " |  \n",
      " |  Tunes word n-grams and generates out-of-sample predictions\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  output_method: string, one of (TaskOutputMethod.PREDICT, TaskOutputMethod.STACK, TaskOutputMethod.PREDICT_MARGIN, TaskOutputMethod.STACK_MARGIN).\n",
      " |  task_parameters: dict, which may contain:\n",
      " |  \n",
      " |    analyzer (analyzer): select, (Default='word')\n",
      " |      Possible Values: ['word', 'char']\n",
      " |  \n",
      " |    binary (bi): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    decode_error (de): select, (Default=2)\n",
      " |      Possible Values: ['strict', 'ignore', 'replace']\n",
      " |  \n",
      " |    encoding (enc): select, (Default=0)\n",
      " |      Possible Values: ['utf-8', 'latin-1']\n",
      " |  \n",
      " |    enet_alpha (a): multi, (Default=0.8)\n",
      " |      Possible Values: {'floatgrid': [0, 1], 'select': ['auto']}\n",
      " |  \n",
      " |    enet_gs_folds (enet_gs_folds): int, (Default=1)\n",
      " |      Possible Values: [1, 100]\n",
      " |  \n",
      " |    enet_gs_fraction (enet_gs_fraction): float, (Default=0.15)\n",
      " |      Possible Values: [1e-05, 0.99999]\n",
      " |  \n",
      " |    enet_gs_light (enet_gs_light): selectgrid, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    enet_n_lambda (nlb): int, (Default=50)\n",
      " |      Possible Values: [2, 100]\n",
      " |  \n",
      " |    enet_stk_seq (enet_stk_seq): selectgrid, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    include_lower (il): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    language (l): select, (Default='english')\n",
      " |      Possible Values: ['arabic', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hindi', 'hungarian', 'italian', 'japanese', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish', 'other']\n",
      " |  \n",
      " |    lemmatizer (lemmatizer): select, (Default=0)\n",
      " |      Possible Values: [None, 'wordnet', 'spacy']\n",
      " |  \n",
      " |    lowercase (lc): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    max_df (madf): multi, (Default=1)\n",
      " |      Possible Values: {'floatgrid': [0.0, 1.0], 'intgrid': [0, 1e+30]}\n",
      " |  \n",
      " |    max_dict_samples (mxd): multi, (Default=None)\n",
      " |      Possible Values: {'intgrid': [1000, 1000000], 'select': [None]}\n",
      " |  \n",
      " |    max_features (mxf): multi, (Default=100000)\n",
      " |      Possible Values: {'select': [None], 'intgrid': [1, 1e+30]}\n",
      " |  \n",
      " |    max_iter (mi): int, (Default=100)\n",
      " |      Possible Values: [1, 1000000]\n",
      " |  \n",
      " |    max_samples (ms): multi, (Default=500)\n",
      " |      Possible Values: {'select': [None], 'float': [1, 10000]}\n",
      " |  \n",
      " |    min_df (midf): multi, (Default=1)\n",
      " |      Possible Values: {'floatgrid': [0.0, 1.0], 'intgrid': [0, 1e+30]}\n",
      " |  \n",
      " |    min_tc (min_tc): int, (Default=0)\n",
      " |      Possible Values: [0, 999]\n",
      " |  \n",
      " |    n_features (nb): int, (Default=262144)\n",
      " |      Possible Values: [1, 1048576]\n",
      " |  \n",
      " |    norm (nrm): select, (Default=0)\n",
      " |      Possible Values: [None, 'l1', 'l2']\n",
      " |  \n",
      " |    num_ngram (num): intgrid, (Default=1)\n",
      " |      Possible Values: [1, 10]\n",
      " |  \n",
      " |    pos_tagging (pos_tagging): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    random_state (rs): int, (Default=1234)\n",
      " |      Possible Values: [0, 1000000000]\n",
      " |  \n",
      " |    segmenter (seg): select, (Default='None')\n",
      " |      Possible Values: [None, 'japanese']\n",
      " |  \n",
      " |    smooth_idf (sidf): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    stemmer (stem): select, (Default=0)\n",
      " |      Possible Values: [None, 'snowball', 'lancaster', 'porter', 'wordnet']\n",
      " |  \n",
      " |    stop_words (sw): multi, (Default=0)\n",
      " |      Possible Values: {'select': [False, True, 'english'], 'intgrid': [0, 1]}\n",
      " |  \n",
      " |    sublinear_tf (sltf): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    svd_algorithm (svd): select, (Default=0)\n",
      " |      Possible Values: [None, 'randomized', 'arpack']\n",
      " |  \n",
      " |    svd_max_components (kmax): int, (Default=1000)\n",
      " |      Possible Values: [10, 1000]\n",
      " |  \n",
      " |    svd_n_components (k): intgrid, (Default=100)\n",
      " |      Possible Values: [1, 100000]\n",
      " |  \n",
      " |    svd_n_iter (ni): int, (Default=2)\n",
      " |      Possible Values: [0, 10]\n",
      " |  \n",
      " |    svd_tol (tol): float, (Default='0.0')\n",
      " |      Possible Values: [0, 1]\n",
      " |  \n",
      " |    tokenizer (tok): select, (Default='sklearn_tokenizer')\n",
      " |      Possible Values: ['sklearn_tokenizer', 'space', 'wordpunct', 'tweet', 'treebank', 'japtiny', 'tiny-segmenter-jp', 'mecab', 'byte_pair_encoding']\n",
      " |  \n",
      " |    tol (e): float, (Default='0.0001')\n",
      " |      Possible Values: [1e-10, 10000000000]\n",
      " |  \n",
      " |    use_bns (bns): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_char_preprocessing (cpp): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_delta (delta): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_hashing (uh): select, (Default=0)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_idf (uidf): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_marisa (um): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |    use_term_frequency (tf): select, (Default=1)\n",
      " |      Possible Values: [False, True]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WNGEC2\n",
      " |      datarobot_bp_workshop.friendly_repr.FriendlyRepr\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(zelf, *inputs, output_method=None, task_parameters=None, output_method_parameters=None, x_transformations=None, y_transformations=None, freeze=False, version=None)\n",
      " |  \n",
      " |  __friendly_repr__(zelf)\n",
      " |  \n",
      " |  documentation(zelf, auto_open=False)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  description = 'Tunes word n-grams and generates out-of-sample predicti...\n",
      " |  \n",
      " |  label = 'Auto-Tuned N-Gram Text Classifier using token counts'\n",
      " |  \n",
      " |  task_code = 'WNGEC2'\n",
      " |  \n",
      " |  task_parameters = analyzer (analyzer): select, (Default='word')\n",
      " |  \n",
      " |  b...1...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from datarobot_bp_workshop.friendly_repr.FriendlyRepr:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from datarobot_bp_workshop.friendly_repr.FriendlyRepr:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understand what each task is doing\n",
    "print(w.Tasks.WNGEC2.documentation())\n",
    "help(w.Tasks.WNGEC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as new blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blueprint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_blueprint:\n",
    "    rfc_blueprint.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see this custom blueprint appear:\n",
    "\n",
    "https://staging.datarobot.com/ai-catalog/user-blueprints/62bb257b6ebefa1737fe070a/graph\n",
    "![view code in GUI](images/blueprint_with_dump_ct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the new blueprint\n",
    "\n",
    "You can do this through the GUI or through CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the dumped file by following [this link](https://docs.datarobot.com/en/docs/modeling/special-workflows/cml/cml-custom-tasks.html#download-training-artifacts) and saving the extracted files here\n",
    "\n",
    "![view code in GUI](images/download_artifacts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"artifact-62bb29ede908f3739ffe0765/dumped.pkl\", 'rb') as fp:\n",
    "    dumped_file = pd.read_pickle(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kycRiskScore</th>\n",
       "      <th>income</th>\n",
       "      <th>tenureMonths</th>\n",
       "      <th>creditScore</th>\n",
       "      <th>nbrPurchases90d</th>\n",
       "      <th>avgTxnSize90d</th>\n",
       "      <th>totalSpend90d</th>\n",
       "      <th>nbrDistinctMerch90d</th>\n",
       "      <th>nbrMerchCredits90d</th>\n",
       "      <th>nbrMerchCreditsRndDollarAmt90d</th>\n",
       "      <th>...</th>\n",
       "      <th>overpaymentInd90d</th>\n",
       "      <th>nbrCustReqRefunds90d</th>\n",
       "      <th>totalRefundsToCust90d</th>\n",
       "      <th>nbrPaymentsCashLike90d</th>\n",
       "      <th>maxRevolveLine</th>\n",
       "      <th>indOwnsHome</th>\n",
       "      <th>nbrInquiries1y</th>\n",
       "      <th>nbrCollections3y</th>\n",
       "      <th>nbrWebLogins90d</th>\n",
       "      <th>nbrPointRed90d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>314.26</td>\n",
       "      <td>7227.98</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.74</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>40200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>772.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>487.43</td>\n",
       "      <td>974.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71700.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.98</td>\n",
       "      <td>231.42</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>100200.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>333.17</td>\n",
       "      <td>57305.24</td>\n",
       "      <td>93.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>119.12</td>\n",
       "      <td>238.24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6395</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11800.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>351.33</td>\n",
       "      <td>4215.96</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.91</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.95</td>\n",
       "      <td>599.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17800.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125.20</td>\n",
       "      <td>250.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>274.24</td>\n",
       "      <td>7130.24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>1.0</td>\n",
       "      <td>45900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.08</td>\n",
       "      <td>176.64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6400 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      kycRiskScore    income  tenureMonths  creditScore  nbrPurchases90d  \\\n",
       "0              0.0   52100.0           1.0        671.0             23.0   \n",
       "1              1.0   40200.0           1.0        772.0              2.0   \n",
       "2              2.0   71700.0           3.0        630.0             29.0   \n",
       "3              1.0  100200.0          31.0        717.0            172.0   \n",
       "4              1.0   21100.0           0.0        716.0              2.0   \n",
       "...            ...       ...           ...          ...              ...   \n",
       "6395           0.0   11800.0          40.0        705.0             12.0   \n",
       "6396           1.0   13000.0          14.0        671.0             20.0   \n",
       "6397           2.0   17800.0          82.0        651.0              2.0   \n",
       "6398           1.0   23900.0           6.0        714.0             26.0   \n",
       "6399           1.0   45900.0           4.0        768.0              8.0   \n",
       "\n",
       "      avgTxnSize90d  totalSpend90d  nbrDistinctMerch90d  nbrMerchCredits90d  \\\n",
       "0            314.26        7227.98                 10.0                 2.0   \n",
       "1            487.43         974.86                  0.0                 0.0   \n",
       "2              7.98         231.42                 18.0                 1.0   \n",
       "3            333.17       57305.24                 93.0                17.0   \n",
       "4            119.12         238.24                  1.0                 0.0   \n",
       "...             ...            ...                  ...                 ...   \n",
       "6395         351.33        4215.96                  5.0                 2.0   \n",
       "6396          29.95         599.00                  9.0                 3.0   \n",
       "6397         125.20         250.40                  1.0                 0.0   \n",
       "6398         274.24        7130.24                  9.0                 1.0   \n",
       "6399          22.08         176.64                  4.0                 1.0   \n",
       "\n",
       "      nbrMerchCreditsRndDollarAmt90d  ...  overpaymentInd90d  \\\n",
       "0                                2.0  ...                0.0   \n",
       "1                                0.0  ...                0.0   \n",
       "2                                0.0  ...                0.0   \n",
       "3                                3.0  ...                0.0   \n",
       "4                                0.0  ...                0.0   \n",
       "...                              ...  ...                ...   \n",
       "6395                             1.0  ...                0.0   \n",
       "6396                             1.0  ...                0.0   \n",
       "6397                             0.0  ...                0.0   \n",
       "6398                             0.0  ...                0.0   \n",
       "6399                             0.0  ...                0.0   \n",
       "\n",
       "      nbrCustReqRefunds90d  totalRefundsToCust90d  nbrPaymentsCashLike90d  \\\n",
       "0                      1.0                  45.74                     3.0   \n",
       "1                      1.0                  40.09                     0.0   \n",
       "2                      1.0                  40.56                     0.0   \n",
       "3                      1.0                  50.75                     4.0   \n",
       "4                      1.0                  44.55                     0.0   \n",
       "...                    ...                    ...                     ...   \n",
       "6395                   1.0                  75.91                     6.0   \n",
       "6396                   1.0                  45.36                     0.0   \n",
       "6397                   1.0                  23.89                     0.0   \n",
       "6398                   1.0                  59.13                     3.0   \n",
       "6399                   1.0                  67.78                     0.0   \n",
       "\n",
       "      maxRevolveLine  indOwnsHome  nbrInquiries1y  nbrCollections3y  \\\n",
       "0            20000.0          1.0             3.0               1.0   \n",
       "1            20000.0          1.0             2.0               0.0   \n",
       "2             8000.0          0.0             5.0               0.0   \n",
       "3            12000.0          1.0             3.0               0.0   \n",
       "4            19000.0          1.0             2.0               0.0   \n",
       "...              ...          ...             ...               ...   \n",
       "6395         11000.0          0.0             4.0               0.0   \n",
       "6396         18000.0          1.0             2.0               0.0   \n",
       "6397         13000.0          1.0             3.0               0.0   \n",
       "6398         17000.0          1.0             2.0               0.0   \n",
       "6399          8000.0          0.0             5.0               0.0   \n",
       "\n",
       "      nbrWebLogins90d  nbrPointRed90d  \n",
       "0                 7.0             1.0  \n",
       "1                 5.0             1.0  \n",
       "2                 8.0             2.0  \n",
       "3                 5.0             1.0  \n",
       "4                 8.0             0.0  \n",
       "...               ...             ...  \n",
       "6395              4.0             0.0  \n",
       "6396              4.0             0.0  \n",
       "6397              5.0             1.0  \n",
       "6398              6.0             1.0  \n",
       "6399              6.0             1.0  \n",
       "\n",
       "[6400 rows x 25 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumped_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast this to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALERT</th>\n",
       "      <th>SAR</th>\n",
       "      <th>kycRiskScore</th>\n",
       "      <th>income</th>\n",
       "      <th>tenureMonths</th>\n",
       "      <th>creditScore</th>\n",
       "      <th>state</th>\n",
       "      <th>nbrPurchases90d</th>\n",
       "      <th>avgTxnSize90d</th>\n",
       "      <th>totalSpend90d</th>\n",
       "      <th>...</th>\n",
       "      <th>indCustReqRefund90d</th>\n",
       "      <th>totalRefundsToCust90d</th>\n",
       "      <th>nbrPaymentsCashLike90d</th>\n",
       "      <th>maxRevolveLine</th>\n",
       "      <th>indOwnsHome</th>\n",
       "      <th>nbrInquiries1y</th>\n",
       "      <th>nbrCollections3y</th>\n",
       "      <th>nbrWebLogins90d</th>\n",
       "      <th>nbrPointRed90d</th>\n",
       "      <th>PEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>110300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>757</td>\n",
       "      <td>PA</td>\n",
       "      <td>10</td>\n",
       "      <td>153.80</td>\n",
       "      <td>1538.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>45.82</td>\n",
       "      <td>5</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>107800.0</td>\n",
       "      <td>6</td>\n",
       "      <td>715</td>\n",
       "      <td>NY</td>\n",
       "      <td>22</td>\n",
       "      <td>1.59</td>\n",
       "      <td>34.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>67.40</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>751</td>\n",
       "      <td>MA</td>\n",
       "      <td>7</td>\n",
       "      <td>57.64</td>\n",
       "      <td>403.48</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>450.69</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "      <td>NJ</td>\n",
       "      <td>14</td>\n",
       "      <td>29.52</td>\n",
       "      <td>413.28</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>71.43</td>\n",
       "      <td>0</td>\n",
       "      <td>8000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>709</td>\n",
       "      <td>PA</td>\n",
       "      <td>54</td>\n",
       "      <td>115.77</td>\n",
       "      <td>6251.58</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2731.39</td>\n",
       "      <td>3</td>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ALERT  SAR  kycRiskScore    income  tenureMonths  creditScore state  \\\n",
       "0      1    0             3  110300.0             5          757    PA   \n",
       "1      1    0             2  107800.0             6          715    NY   \n",
       "2      1    0             1   74000.0            13          751    MA   \n",
       "3      1    0             0   57700.0             1          659    NJ   \n",
       "4      1    0             1   59800.0             3          709    PA   \n",
       "\n",
       "   nbrPurchases90d  avgTxnSize90d  totalSpend90d  ... indCustReqRefund90d  \\\n",
       "0               10         153.80        1538.00  ...                   1   \n",
       "1               22           1.59          34.98  ...                   1   \n",
       "2                7          57.64         403.48  ...                   1   \n",
       "3               14          29.52         413.28  ...                   1   \n",
       "4               54         115.77        6251.58  ...                   1   \n",
       "\n",
       "   totalRefundsToCust90d  nbrPaymentsCashLike90d  maxRevolveLine  indOwnsHome  \\\n",
       "0                  45.82                       5            6000            0   \n",
       "1                  67.40                       0           10000            1   \n",
       "2                 450.69                       0           10000            0   \n",
       "3                  71.43                       0            8000            1   \n",
       "4                2731.39                       3            7000            1   \n",
       "\n",
       "   nbrInquiries1y  nbrCollections3y  nbrWebLogins90d  nbrPointRed90d  PEP  \n",
       "0               3                 0                6               1    0  \n",
       "1               3                 0               87               0    0  \n",
       "2               3                 0                6               0    0  \n",
       "3               5                 0                7               2    0  \n",
       "4               1                 0                8               1    0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that the other data types were filtered out, and that missing values were imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible extensions\n",
    "\n",
    "- Add parameters to specify file name\n",
    "- Save in other formats than pickle\n",
    "- enable non-tabular data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('composable-ml-demo': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ca763f306d6ad9fd8c5f50b21e9e3912817176f4f539983f08654cefafb7c5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
